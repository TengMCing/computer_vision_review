[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Computer Vision Paper Review",
    "section": "",
    "text": "Dr Patrick (Weihao) Li is a Postdoctoral Research Fellow at the Biological Data Science Institute, Australian National University. He holds a Bachelor of Commerce (majoring in Business Analytics), an Honours degree in Econometrics, and a PhD in Statistics from the Department of Econometrics and Business Statistics at Monash University.\nHis research focuses on machine learning, data visualization, visual inference, statistical software development, and computer vision, with a particular interest in automating the evaluation of diagnostic graphics and understanding how people interpret them.\nPatrick has developed and published multiple R packages on CRAN and GitHub, contributing to the open-source ecosystem for data analysis and visualization. He is a strong advocate for open and reproducible science, and regularly integrates tools such as Git, Quarto, and Jupyter into his workflows. He is proficient in R and Python, with growing experience in C and systems-level programming. He is also a member of the Statistical Society of Australia (SSA).\n \n  \n   \n  \n    \n     My website\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Google Scholar\n  \n  \n    \n     ORCID"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "Computer Vision Paper Review",
    "section": "Contact",
    "text": "Contact\n\n patrick.li@anu.edu.edu\n 46 Sullivanâ€™s Creek Road, R.N. Robertson Building, The Australian National University, ACT 2600 Australia"
  },
  {
    "objectID": "slides/yann1998.html#gradient-based-learning-applied-to-document-recognition",
    "href": "slides/yann1998.html#gradient-based-learning-applied-to-document-recognition",
    "title": "Computer Vision Paper Review",
    "section": "Gradient-based learning applied to document recognition",
    "text": "Gradient-based learning applied to document recognition\nY. Lecun; L. Bottou; Y. Bengio; P. Haffner (1998)\n\nHistorical Paper Review\n\nPatrick Li\n\nBDSI, ANU"
  },
  {
    "objectID": "slides/yann1998.html#overview",
    "href": "slides/yann1998.html#overview",
    "title": "Computer Vision Paper Review",
    "section": "Overview",
    "text": "Overview\n\nThe 1998 computer vision landscape\nThe problem with traditional pattern recognition\nGradient-based learning revolution\nConvolutional neural network foundations\nLeNet-5\nBeyond LeNet: Graph transformer networks\nGTN Applications"
  },
  {
    "objectID": "slides/yann1998.html#the-1998-computer-vision-landscape",
    "href": "slides/yann1998.html#the-1998-computer-vision-landscape",
    "title": "Computer Vision Paper Review",
    "section": "The 1998 computer vision landscape",
    "text": "The 1998 computer vision landscape\n\n\n\n\n\n\n%%{init: {'themeVariables': { 'fontSize': '20px'}}}%%\ntimeline\n    title Evolution of Pattern Recognition (Pre-1998)\n    1950s : Early Pattern Recognition in Cybernetics : Perceptron\n    1960s : Linear Classifiers & Perceptrons dominate : Expose limitations (XOR problem), triggering \"AI winter\" for neural networks\n    1970s : Statistical Decision Theory (Bayesian methods, k-NN, Fisher discriminant analysis) : Hidden Markov Models (HMMs)\n    1980-1984 : Symbolic AI & Expert Systems peak : Hand-crafted feature engineering dominates : Neocognitron introduced (early CNN)\n    1985-1989 : Backpropagation popularized : Neural networks revived\n    1990s : Support Vector Machines (SVMs) : Neural Networks struggle (Limited compute/data, but backpropagation improves) : Boosting algorithms\n    1998 : LeNet-5 (First practical CNN for digit recognition) : End-to-End Learning"
  },
  {
    "objectID": "slides/yann1998.html#traditional-pattern-recognition",
    "href": "slides/yann1998.html#traditional-pattern-recognition",
    "title": "Computer Vision Paper Review",
    "section": "Traditional pattern recognition",
    "text": "Traditional pattern recognition\n\n\n\n\n\n\n\n%%{init: {'themeVariables': { 'fontSize': '15px'}}}%%\ngraph TD\n    A[\"Input Image\"] --&gt; B[\"Feature Extractor &lt;br&gt; (Handcrafted, Task-Specific)\"]\n    B --&gt; C[\"Low-Dimensional Representation &lt;br&gt; (Invariant to Distortions)\"]\n    C --&gt; D[\"Classifier &lt;br&gt; (General-Purpose, Trainable)\"]\n    D --&gt; E[\"Prediction / Label\"]\n\n    subgraph \" \"\n        B\n        C\n        D\n    end\n\n\n\n\n\n\n\n\nTwo-module system\n\nFeature Extractor\nClassifier\n\n\n\n\n\n\n\nProblem\n\n\nRecognition accuracy heavily depends on the designerâ€™s ability to create good features, which is time-consuming and problem-specific."
  },
  {
    "objectID": "slides/yann1998.html#challenges-in-handwriting-recognition",
    "href": "slides/yann1998.html#challenges-in-handwriting-recognition",
    "title": "Computer Vision Paper Review",
    "section": "Challenges in handwriting recognition",
    "text": "Challenges in handwriting recognition\nSeparating characters (Segmentation) within words/sentences is a major challenge.\n\n\n\n\n\n\n\nReal world handwriting will be 100x worse than this.\n\n\n\n\n\n\n\n\n\n\n\nHeuristic Oversegmentation (HOS)\n\n\nGenerates potential cuts between characters based on heuristics, then selects best combination based on recognizer scores."
  },
  {
    "objectID": "slides/yann1998.html#shift-to-learning-based-systems",
    "href": "slides/yann1998.html#shift-to-learning-based-systems",
    "title": "Computer Vision Paper Review",
    "section": "Shift to learning-based systems",
    "text": "Shift to learning-based systems\n\n\n\n\n\n%%{init: {'themeVariables': { 'fontSize': '20px'}}}%%\ngraph TB\n        subgraph \"Learning-Based Approach\"\n        F[Raw Image] --&gt; G[End-to-End&lt;br/&gt;Learning System]\n        G --&gt; H[Output]\n        G -.-&gt; I[(\"ğŸ¤– Automatic&lt;br/&gt;Feature Discovery\")]\n    end\n    \n    subgraph \"Traditional Approach\"\n        A[Raw Image] --&gt; B[Hand-crafted&lt;br/&gt;Feature Extractor]\n        B --&gt; C[Trainable&lt;br/&gt;Classifier]\n        C --&gt; D[Output]\n        B -.-&gt; E[(\"ğŸ‘¨â€ğŸ’» Human Expert&lt;br/&gt;Designs Features\")]\n    end\n    \n    style E fill:#ffcccc\n    style I fill:#ccffcc\n\n\n\n\n\n\n\nFaster hardware enables brute-force numerical methods.\nLarge datasets (e.g., MNIST) allow reliance on real data.\nPowerful learning techniques handle high-dimensional inputs and complex decision functions."
  },
  {
    "objectID": "slides/yann1998.html#gradient-based-learning",
    "href": "slides/yann1998.html#gradient-based-learning",
    "title": "Computer Vision Paper Review",
    "section": "Gradient-based learning",
    "text": "Gradient-based learning\n\n\n\n\n\n%%{init: {'themeVariables': { 'fontSize': '25px'}}}%%\ngraph LR\n    A[Input Z] --&gt; B[Learning Machine F]\n    B --&gt; C[Output Y]\n    D[Parameters W] --&gt; B\n    E[Desired Output D] --&gt; F[Loss Function E]\n    C --&gt; F\n    F --&gt; G[\"Gradient $$\\;\\partial E/ \\partial W$$\"]\n    G --&gt; H[Parameter Update]\n    H --&gt; D\n    \n    style F fill:#ffffcc\n    style G fill:#ccffff\n\n\n\n\n\n\n\nLearning machine computes: \\(Y^p = F(Z^p, W)\\), where\\(Z^p\\) is input, \\(W\\) is parameters.\nLoss function: \\(E^p = D(D^p, F(W, Z^p))\\), measuring discrepancy between desired and actual output.\nAverage loss: \\(E_{\\text{train}}(W)\\) over training set."
  },
  {
    "objectID": "slides/yann1998.html#generalization-gap",
    "href": "slides/yann1998.html#generalization-gap",
    "title": "Computer Vision Paper Review",
    "section": "Generalization gap",
    "text": "Generalization gap\n\\[E_{\\text{test}} - E_{\\text{train}} = k(h/P)^\\alpha,\\] where \\(P\\) is the number of training samples, \\(h\\) is the model capacity, \\(\\alpha \\in [0.5, 1]\\) and \\(k \\in \\mathbb{R}\\) are constants.\n\n\n\n\n\n\nTradeoff\n\n\nIncreasing capacity reduces \\(E_{\\text{train}}\\) but increases the gap. Optimal capacity minimizes \\(E_{\\text{test}}\\)."
  },
  {
    "objectID": "slides/yann1998.html#structural-risk-minimization",
    "href": "slides/yann1998.html#structural-risk-minimization",
    "title": "Computer Vision Paper Review",
    "section": "Structural risk minimization",
    "text": "Structural risk minimization\n\\[\\arg\\min_{W} \\; E_{\\text{train}} + \\beta H(W),\\] where \\(H(W)\\) penalizes high-capacity parameters (regularization) and \\(\\beta\\) is a constant.\n\n\n\n\n\n\nIn practice, structural risk minimization is usually achieved by:\n\n\n\nUsing models of increasing complexity (e.g., polynomial degree, network size)\nAdding regularization penalties (e.g., L1/L2 regularization)\nSelecting models via validation\nâ€¦"
  },
  {
    "objectID": "slides/yann1998.html#optimization-techniques",
    "href": "slides/yann1998.html#optimization-techniques",
    "title": "Computer Vision Paper Review",
    "section": "Optimization techniques",
    "text": "Optimization techniques\n\n\n\n\n\n\n%%{init: {'themeVariables': { 'fontSize': '18px'}}}%%\ngraph TD\n    A[Optimization]\n\n    A --&gt; B[Gradient Descent]\n    B --&gt; B1[\"$$W_k = W_{k-1} - \\varepsilon \\; \\frac{\\partial E(W)}{\\partial W}$$\"]\n    B --&gt; B2[\"`Îµ can be scalar, \n    matrix, or adaptive`\"]\n    B2 --&gt; B2a[Examples: &lt;br&gt;quasi-Newton methods]\n\n    A --&gt; C[Stochastic Gradient]\n    C --&gt; C1[\"$$W_k = W_{k-1} - \\varepsilon \\; \\frac{\\partial E^{p_k}(W)}{\\partial W}$$\"]\n    C --&gt; C2[Faster convergence for large, &lt;br&gt;redundant datasets]\n    C2 --&gt; C2a[Examples: speech, &lt;br&gt;character recognition]"
  },
  {
    "objectID": "slides/yann1998.html#popularization",
    "href": "slides/yann1998.html#popularization",
    "title": "Computer Vision Paper Review",
    "section": "Popularization",
    "text": "Popularization\n\nThe popularization of gradient-based learning was driven by three key events:\n\nLocal minima were found not to be a major issue in practice.\nBackpropagation algorithm was introduced as a simple and efficient method to compute gradients in multilayer nonlinear systems.\nBackpropagation + multilayer neural networks + sigmoidal units was demonstrated to effectively solve complex learning problems."
  },
  {
    "objectID": "slides/yann1998.html#backpropagation",
    "href": "slides/yann1998.html#backpropagation",
    "title": "Computer Vision Paper Review",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nGradients are computed by propagating errors backward from the output layer to the input layer, using the chain rule.\n\\[\\begin{align*}\n\\frac{\\partial E^p}{\\partial W_n} &= \\frac{\\partial F}{\\partial W}(W_n, X_{n-1})\\frac{\\partial E^p}{\\partial X_n} \\\\\n\\frac{\\partial E^p}{\\partial X_{n-1}} &= \\frac{\\partial F}{\\partial X}(W_n, X_{n-1})\\frac{\\partial E^p}{\\partial X_n}, \\\\\n\\end{align*}\\]\nwhere \\(W_n\\) and \\(X_n\\) denote the weights and outputs of the \\(n\\)th layer, respectively. And \\(X_n = F_n(W_n, X_{n-1})\\).\n\n\n\n\n\n\n\n3Blue1Brown\n\n\n\nBackpropagation, intuitively | DL3\nBackpropagation calculus | DL4"
  },
  {
    "objectID": "slides/yann1998.html#why-fully-connected-nns-struggle-with-character-recognition",
    "href": "slides/yann1998.html#why-fully-connected-nns-struggle-with-character-recognition",
    "title": "Computer Vision Paper Review",
    "section": "Why fully connected NNs struggle with character recognition?",
    "text": "Why fully connected NNs struggle with character recognition?\n\nHigh dimensionality: Large input sizes lead to an explosion of parameters.\n\n\n\nModel: \"functional\"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ Layer (type)                    â”ƒ Output Shape           â”ƒ       Param # â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (InputLayer)        â”‚ (None, 4096)           â”‚             0 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (Dense)                   â”‚ (None, 256)            â”‚     1,048,832 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (Dense)                 â”‚ (None, 2)              â”‚           514 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n Total params: 1,049,346 (4.00 MB)\n Trainable params: 1,049,346 (4.00 MB)\n Non-trainable params: 0 (0.00 B)"
  },
  {
    "objectID": "slides/yann1998.html#why-fully-connected-nns-struggle-with-character-recognition-1",
    "href": "slides/yann1998.html#why-fully-connected-nns-struggle-with-character-recognition-1",
    "title": "Computer Vision Paper Review",
    "section": "Why fully connected NNs struggle with character recognition?",
    "text": "Why fully connected NNs struggle with character recognition?\n\nNo translation invariance: The same object in a different place looks completely different.\n\n\nkeras_mod$predict(verbar, verbose = 0L)\n\n          [,1]      [,2]\n[1,] 0.8466364 0.1533636\n\nkeras_mod$predict(verbar_shift, verbose = 0L)\n\n          [,1]      [,2]\n[1,] 0.6208922 0.3791077"
  },
  {
    "objectID": "slides/yann1998.html#why-fully-connected-nns-struggle-with-character-recognition-2",
    "href": "slides/yann1998.html#why-fully-connected-nns-struggle-with-character-recognition-2",
    "title": "Computer Vision Paper Review",
    "section": "Why fully connected NNs struggle with character recognition?",
    "text": "Why fully connected NNs struggle with character recognition?\n\nIgnored input topology: The spatial structure of images and local correlations are not inherently utilized."
  },
  {
    "objectID": "slides/yann1998.html#convolutional-neural-networks",
    "href": "slides/yann1998.html#convolutional-neural-networks",
    "title": "Computer Vision Paper Review",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\nA convolutional neural network typically consists of:\n\nConvolutional layers\nSubsampling (or pooling) layers\nFully connected layers"
  },
  {
    "objectID": "slides/yann1998.html#local-receptive-fields",
    "href": "slides/yann1998.html#local-receptive-fields",
    "title": "Computer Vision Paper Review",
    "section": "Local receptive fields",
    "text": "Local receptive fields\n\n\nConvolutional layers connect each unit to a small local region of the previous layer.\nLocal receptive fields extract basic features (e.g.Â edges, corners), which are combined in deeper layers to detect higher order patterns."
  },
  {
    "objectID": "slides/yann1998.html#weight-sharing",
    "href": "slides/yann1998.html#weight-sharing",
    "title": "Computer Vision Paper Review",
    "section": "Weight sharing",
    "text": "Weight sharing"
  },
  {
    "objectID": "slides/yann1998.html#weight-sharing-1",
    "href": "slides/yann1998.html#weight-sharing-1",
    "title": "Computer Vision Paper Review",
    "section": "Weight sharing",
    "text": "Weight sharing\n\n\\[\\text{Weight sharing} = \\text{shift invariance} + \\text{less parameters} + \\text{smaller generalization gap}\\]\n\n\nLocal feature detectors can be applied across the image using identical weights.\nMultiple filters extract different features at each location.\nShifted inputs produce shifted feature maps, making CNNs robust to shifts and distortions.\nWeight sharing reduces model capacity, helping to reduce the train-test error gap."
  },
  {
    "objectID": "slides/yann1998.html#spatial-subsampling",
    "href": "slides/yann1998.html#spatial-subsampling",
    "title": "Computer Vision Paper Review",
    "section": "Spatial subsampling",
    "text": "Spatial subsampling\n\nExact feature location is less important than relative position. Pooling reduces resolution and sensitivity to shifts and distortions by averaging over local neighbourhoods."
  },
  {
    "objectID": "slides/yann1998.html#lenet-5",
    "href": "slides/yann1998.html#lenet-5",
    "title": "Computer Vision Paper Review",
    "section": "LeNet-5",
    "text": "LeNet-5\n\n\n\n\n\n%%{init: {'themeVariables': { 'fontSize': '33px'}}}%%\ngraph LR\n    Input[\"Input&lt;br/&gt;32Ã—32Ã—1&lt;br/&gt;Grayscale Image\"] --&gt; C1[\"Convolution&lt;br/&gt;6 filters, 5Ã—5&lt;br/&gt;Output: &lt;br/&gt;28Ã—28Ã—6\"]\n    C1 --&gt; S2[\"Subsampling&lt;br/&gt;Average Pooling 2Ã—2&lt;br/&gt;Output: &lt;br/&gt;14Ã—14Ã—6\"]\n    S2 --&gt; C3[\"Convolution&lt;br/&gt;16 filters, 5Ã—5&lt;br/&gt;Output: &lt;br/&gt;10Ã—10Ã—16\"]\n    C3 --&gt; S4[\"Subsampling&lt;br/&gt;Average Pooling 2Ã—2&lt;br/&gt;Output: &lt;br/&gt;5Ã—5Ã—16\"]\n    S4 --&gt; C5[\"Convolution&lt;br/&gt;120 filters, 5Ã—5&lt;br/&gt;Output: &lt;br/&gt;1Ã—1Ã—120&lt;br/&gt;\"]\n    C5 --&gt; F6[\"Dense&lt;br/&gt;84 units&lt;br/&gt;tanh activation\"]\n    F6 --&gt; Output[\"Output Layer&lt;br/&gt;10 units&lt;br/&gt;Classes 0-9\"]\n\n    classDef convLayer fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef poolLayer fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n    classDef fcLayer fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef inputOutput fill:#fff3e0,stroke:#e65100,stroke-width:2px\n\n    class Input,Output inputOutput\n    class C1,C3,C5 convLayer\n    class S2,S4 poolLayer\n    class F6 fcLayer\n\n\n\n\n\n\n\n\n\n\n\n\nMain characteristics\n\n\n\nFeature maps increases as resolution decreases, building invariance through richer representations.\nStandardised inputs accelerates training.\nNo padding is used, so feature maps shrink after convolution.\nThe final layer compares against 10 prototype vectors (RBF units) using Euclidean distance (prototype-based classification). It will produce low activations for out-of-distribution inputs."
  },
  {
    "objectID": "slides/yann1998.html#discriminative-map-criterion",
    "href": "slides/yann1998.html#discriminative-map-criterion",
    "title": "Computer Vision Paper Review",
    "section": "Discriminative MAP criterion",
    "text": "Discriminative MAP criterion\nMaximizes the posterior probability of the correct class while penalizing incorrect classes.\n\\[E(W) = \\frac{1}{P} \\sum_{p=1}^{P} \\left( y_{D^p}(Z^p, W) + \\log \\left( e^{-j} + \\sum_{i} e^{-y_i(Z^p, W)} \\right) \\right)\\]\n\nThe output layer uses RBF units, where a lower \\(y_{D^p}\\) indicates better matching.\nThe term \\(e^{-j}\\) accounts for a â€œrubbishâ€ class to reject non-character inputs, where \\(j\\) is a positive constant."
  },
  {
    "objectID": "slides/yann1998.html#results",
    "href": "slides/yann1998.html#results",
    "title": "Computer Vision Paper Review",
    "section": "Results",
    "text": "Results\n\nLeNet-5 achieves high accuracy on MNIST, with LeNet-1 outperforming larger fully connected networks.\nData augmentation and boosting (e.g., boosted LeNet-4) significantly improve performance.\nCNNs scale well with data and preserve spatial structure, outperforming SVMs and KNN-based methods like tangent distance classifier.\nSVMs perform surprisingly well despite lacking spatial priors, but are less efficient on large datasets."
  },
  {
    "objectID": "slides/yann1998.html#multimodule-systems",
    "href": "slides/yann1998.html#multimodule-systems",
    "title": "Computer Vision Paper Review",
    "section": "Multimodule systems",
    "text": "Multimodule systems\n\n\n\n\n\n%%{init: {'themeVariables': { 'fontSize': '30px'}}}%%\ngraph LR\n    A[Input Data] --&gt; B[Module 1: &lt;br/&gt;Neural Network]\n    A --&gt; C[Module 2: &lt;br/&gt;Frozen &lt;br/&gt;Embedding]\n    A --&gt; D[Module 3: &lt;br/&gt;Nondifferentiable &lt;br/&gt;Function]\n    \n    B --&gt; E[Intermediate &lt;br/&gt;Output 1]\n    C --&gt; F[Intermediate &lt;br/&gt;Output 2]\n    D --&gt; G[Intermediate &lt;br/&gt;Output 3]\n    \n    E --&gt; H[Combiner &lt;br/&gt;Module]\n    F --&gt; H\n    G --&gt; H\n    \n    H --&gt; I[Final Output]\n    I --&gt; J[Loss Function]\n    \n    J -.-&gt;|âˆ‚L/âˆ‚Î¸| B\n    J -.-&gt;|No gradients| C\n    J -.-&gt;|âˆ‚L/âˆ‚Î¸ &lt;br/&gt; via &lt;br/&gt;differentiable &lt;br/&gt;params| D\n    classDef trainable fill:#e1f5fe,stroke:#01579b,stroke-width:2px\n    classDef frozen fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n    classDef nondiff fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef gradient stroke:#d32f2f,stroke-width:3px,stroke-dasharray: 5 5\n    \n    class B,H,K trainable\n    class C,L frozen\n    class D,M nondiff\n    class J gradient\n\n\n\n\n\n\nGradient-based learning (backpropagation) can be generalized to systems composed of heterogeneous modules, not just traditional neural networks."
  },
  {
    "objectID": "slides/yann1998.html#graph-transformer-network-gtn",
    "href": "slides/yann1998.html#graph-transformer-network-gtn",
    "title": "Computer Vision Paper Review",
    "section": "Graph transformer network (GTN)",
    "text": "Graph transformer network (GTN)\n\nVariable-length inputs can be represented using directed graphs, where each path corresponds to a possible sequence and arcs carry vectors containing information such as probabilities, features, or penalties.\n\n\n\n\n\n\n\nIn GTN, each module takes graph as input and output a graph."
  },
  {
    "objectID": "slides/yann1998.html#segmentation-graph",
    "href": "slides/yann1998.html#segmentation-graph",
    "title": "Computer Vision Paper Review",
    "section": "Segmentation graph",
    "text": "Segmentation graph\n\nHOS is a traditional segmentation method that generates candidate cuts using heuristics and selects the best one.\n\nnodes represent cuts\narcs represent ink segments between cuts that could form characters"
  },
  {
    "objectID": "slides/yann1998.html#recognition-transformer",
    "href": "slides/yann1998.html#recognition-transformer",
    "title": "Computer Vision Paper Review",
    "section": "Recognition transformer",
    "text": "Recognition transformer\n\n\n\nThe recognition transformer converts the segmentation graph into an interpretation graph.\nIt processes each segmentation arc to generate all possible interpretations.\nPenalties from segmentation and recognition are combined on interpretation arcs."
  },
  {
    "objectID": "slides/yann1998.html#viterbi-transformer",
    "href": "slides/yann1998.html#viterbi-transformer",
    "title": "Computer Vision Paper Review",
    "section": "Viterbi transformer",
    "text": "Viterbi transformer\n\n\n\nThe Viterbi transformer extracts the best interpretation using dynamic programming.\nIt finds the lowest-penalty path in the interpretation graph (Viterbi path)."
  },
  {
    "objectID": "slides/yann1998.html#constrained-interpretation-graph",
    "href": "slides/yann1998.html#constrained-interpretation-graph",
    "title": "Computer Vision Paper Review",
    "section": "Constrained interpretation graph",
    "text": "Constrained interpretation graph\n\nDuring training, a path selector is inserted between the interpretation graph and the Viterbi transformer.\nThis selector filters for all paths that contain the correct label sequence, producing what is known as the constrained interpretation graph."
  },
  {
    "objectID": "slides/yann1998.html#viterbi-training",
    "href": "slides/yann1998.html#viterbi-training",
    "title": "Computer Vision Paper Review",
    "section": "Viterbi training",
    "text": "Viterbi training\n\n\n\nViterbi algorithm finds the best path, and we want it close to the actual label sequence.\nLoss = sum of penalties on the Viterbi-selected path.\nOnly the final path receives gradient updates; others are zero.\n\n\n\n\n\n\nCompeting low-penalty paths are ignored."
  },
  {
    "objectID": "slides/yann1998.html#discriminative-viterbi-training",
    "href": "slides/yann1998.html#discriminative-viterbi-training",
    "title": "Computer Vision Paper Review",
    "section": "Discriminative Viterbi training",
    "text": "Discriminative Viterbi training\n\n\n\n\nCompute the difference in penalties between the Viterbi path and the constrained Viterbi path, and use it as a loss term.\nThis penalizes incorrect predictions, but lacks a margin between classes. Once the penalties match, the gradient vanishes.\nIdeally, wrong paths should be penalized more if they are too close to the correct one."
  },
  {
    "objectID": "slides/yann1998.html#discriminative-forward-training",
    "href": "slides/yann1998.html#discriminative-forward-training",
    "title": "Computer Vision Paper Review",
    "section": "Discriminative forward training",
    "text": "Discriminative forward training\n\n\n\n\n\nComputes the â€œsoftâ€ version of the min function over all path penalties in both the constrained graph and unconstrained graph:\n\\[f_n = \\text{logadd}_{i \\in U_n}(c_i + f_{s_i}),\\] where \\(\\text{logadd}(x_1, x_2, ..., x_n) = -\\log(\\sum_{i=1}^n e^{-x_i})\\), \\(U_n\\) denotes the set of upstream arcs of node \\(n\\), \\(c_i\\) is the penalty on arc \\(i\\), and \\(s_i\\) is the source node of the arc.\nUnlike Viterbi, which selects the best path, this smoothly incorporates all possible paths into the computation."
  },
  {
    "objectID": "slides/yann1998.html#space-displacement-neural-network-sdnn",
    "href": "slides/yann1998.html#space-displacement-neural-network-sdnn",
    "title": "Computer Vision Paper Review",
    "section": "Space displacement neural network (SDNN)",
    "text": "Space displacement neural network (SDNN)\n\n\n\n\n\nWe can avoid segmentation heuristics by applying a CNN at every position in the image, similar to how filters operate, to produce character probabilities at each location.\nThis output is then fed into a Character Model Transducer to construct the interpretation graph."
  },
  {
    "objectID": "slides/yann1998.html#space-displacement-neural-network-sdnn-1",
    "href": "slides/yann1998.html#space-displacement-neural-network-sdnn-1",
    "title": "Computer Vision Paper Review",
    "section": "Space displacement neural network (SDNN)",
    "text": "Space displacement neural network (SDNN)\n\n\n\n\n\nSDNN can be used for brute-force object detection by training a CNN to output the probability of an object (e.g., a dog) in a fixed-size image.\nSweeping this CNN over a large image produces a 2D probability heatmap.\nRepeating this at multiple scales allows detection of objects at different sizes."
  },
  {
    "objectID": "slides/yann1998.html#amap-representation",
    "href": "slides/yann1998.html#amap-representation",
    "title": "Computer Vision Paper Review",
    "section": "AMAP representation",
    "text": "AMAP representation\n\nAMAP encodes pen trajectories as low-resolution images, with each pixel holding a five-element feature vector: four features are associated to four orientations of the pen trajectory in the area around the pixel and one for curvature. It is invariant to stroke order and writing speed."
  },
  {
    "objectID": "slides/yann1998.html#ex.1-online-handwriting-recognition",
    "href": "slides/yann1998.html#ex.1-online-handwriting-recognition",
    "title": "Computer Vision Paper Review",
    "section": "Ex.1: online handwriting recognition",
    "text": "Ex.1: online handwriting recognition"
  },
  {
    "objectID": "slides/yann1998.html#ex.2-check-reading-system",
    "href": "slides/yann1998.html#ex.2-check-reading-system",
    "title": "Computer Vision Paper Review",
    "section": "Ex.2: check reading system",
    "text": "Ex.2: check reading system\n\n\n\n\n\n\n\n\n\n\n\nWorkflow\n\n\n\nIdentify fields likely to contain the courtesy amount.\nSegment each field into characters, score interpretations, and select the most probable field and amount."
  },
  {
    "objectID": "slides/yann1998.html#takeaways",
    "href": "slides/yann1998.html#takeaways",
    "title": "Computer Vision Paper Review",
    "section": "Takeaways",
    "text": "Takeaways\n\n\nNeural networks can handle inputs and outputs of varying structure (e.g.Â sequences, graphs) as long as the computation is differentiable. The model parameters can remain fixed in size.\nThe same training approach applies when choosing between discrete interpretations, such as in structured prediction tasks.\nThis 1998 paper laid the foundation for modern deep learning systems.\n\n\nReplacing handcrafted features with end-to-end learning\nLeveraging spatial structure via convolutional design\nScaling models effectively, shown by LeNet-5â€™s success in document recognition."
  },
  {
    "objectID": "slides/yann1998.html#thanks-any-questions",
    "href": "slides/yann1998.html#thanks-any-questions",
    "title": "Computer Vision Paper Review",
    "section": "Thanks! Any questions?",
    "text": "Thanks! Any questions?\n\n\n TengMCing/computer_vision_review/slides\n TengMCing\n patrick.li@anu.edu.au\n https://patrickli.org\n\n\n\n\n\nSlides URL: https://cv-review.patrickli.org | Canberra time"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Computer Vision Paper Review series!",
    "section": "",
    "text": "Welcome to the Computer Vision Paper Review series!\n\n\n\n\n\n\n\nExplore the evolution of computer vision research from the early days of convolutional neural networks to the cutting-edge transformer models. Along the way you will find insights into key tasks such as object detection, segmentation, and image generation.\nThis collection is continuously updated with papers presented as Quarto slides. It is designed primarily as a structured learning resource for myself but I hope you find it helpful as well.\nIf you spot any mistakes or have suggestions please let me know by reporting an issue here: https://github.com/TengMCing/computer_vision_review/issues\nThank you for visiting and happy learning."
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Slides",
    "section": "",
    "text": "Gradient-based learning applied to document recognition\n\n\n\n\n\n\nNeural networks\n\n\nCharacter recognition\n\n\n1998\n\n\n\nThis slide summarizes the landmark 1998 paper by Yann LeCun and colleagues, which introduced the LeNet-5 convolutional neural network architecture and demonstrated its effectiveness for handwritten digit recognition. The paper established convolutional neural networks (CNNs) as powerful models for image-based tasks, highlighting their ability to learn hierarchical features directly from raw pixels using gradient-based training. By applying CNNs to document recognition, the authors set a new benchmark on the MNIST dataset and laid the groundwork for the modern deep learning revolution. \n\n\n\n\n\nMay 28, 2025\n\n\n\n\n\n\nNo matching items"
  }
]